# Context Limits Findings - Qwen2.5.1 Coder 7B

## Test Results Summary

Comprehensive testing to find the breaking point for the 7B model on RTX 5060 Ti (16GB).

| Context Size | Messages | Actual Tokens | Processing Time | Generation | Status |
|-------------|----------|---------------|-----------------|------------|---------|
| **2K** | 22 | ~2,000 | ~10 seconds | ‚úÖ 0.64s | ‚ö° **EXCELLENT** |
| **10K** | 102 | 7,523 | ~4 seconds | ‚ùå HUNG | **FAILED** |
| **20K** | 202 | 15,059 | ~8 seconds | ‚ùå HUNG | **FAILED** |
| **32K** | 322 | 24,221 | ~10 seconds | ‚ùå HUNG | **FAILED** |

## Critical Finding

**Hard breakpoint between 2k-10k tokens:**
- ‚úÖ Under ~2k tokens: **Fast, reliable responses** (<1 second)
- ‚ùå Above ~7k tokens: **Generation phase hangs indefinitely**

This is **not a performance issue** - it's a **hard failure mode**. The model:
1. Successfully processes prompts up to 24k+ tokens
2. Completes sampler initialization
3. **Then hangs forever** when trying to generate output

## Detailed Test Logs

### ‚úÖ 2K Test - SUCCESS
```
üìä Messages: 22
üìä Tokens: ~2,000
‚è±Ô∏è  Processing: Fast
‚è±Ô∏è  Generation: 0.64 seconds
‚úÖ Reply: "Response'm sorry, but'm sorry, but don't't to continue this conversation."
üìà Status: EXCELLENT
```

### ‚ùå 10K Test - HUNG
```
üìä Messages: 102
üìä Tokens: 7,523 (actual)
‚è±Ô∏è  Prompt processing: ~4 seconds ‚úÖ
‚è±Ô∏è  Sampler init: 0.83ms ‚úÖ
‚è±Ô∏è  Generation: HUNG (30+ seconds, killed) ‚ùå

Logs:
slot update_slots: prompt done, n_tokens = 7523
slot init_sampler: took 0.83 ms
[NO FURTHER OUTPUT - HUNG]
```

### ‚ùå 20K Test - HUNG
```
üìä Messages: 202
üìä Tokens: 15,059 (actual)
‚è±Ô∏è  Prompt processing: ~8 seconds ‚úÖ
‚è±Ô∏è  Sampler init: 1.56ms ‚úÖ
‚è±Ô∏è  Generation: HUNG (45+ seconds, killed) ‚ùå

Same pattern - hangs after sampler init
```

### ‚ùå 32K Test - HUNG
```
üìä Messages: 322
üìä Tokens: 24,221 (actual)
‚è±Ô∏è  Prompt processing: ~10 seconds ‚úÖ
‚è±Ô∏è  Sampler init: 2.49ms ‚úÖ
‚è±Ô∏è  Generation: HUNG (60+ seconds, killed) ‚ùå

Same pattern - hangs after sampler init
```

## Root Cause Analysis

### What Works
- ‚úÖ Prompt processing (even for 24k+ tokens)
- ‚úÖ Sampler initialization
- ‚úÖ Model loading and GPU utilization

### What Fails
- ‚ùå **Token generation with large context**
- The model enters a hung state after sampler init
- No error messages - just infinite wait
- CPU usage stable but no progress

### Likely Cause
**KV cache overflow or attention mechanism breakdown** at generation time:
- The model can **read** large contexts
- But **cannot generate** with them in the cache
- Possibly related to:
  - KV cache quantization (q4_0) limitations
  - Attention computation complexity (O(n¬≤))
  - Memory bandwidth constraints
  - Flash attention implementation limits

## Recommendations

### For OpenClaw Integration

**Set `contextWindow: 3072` (3k tokens):**
```json
{
  "contextWindow": 3072,
  "maxTokens": 1024
}
```

This provides:
- ‚úÖ Fast responses (<1 second)
- ‚úÖ Decent conversation history (~15-20 message pairs)
- ‚úÖ Reliable operation (no hangs)
- ‚úÖ Room for system prompts and tools

### For Larger Context Needs

**Options:**
1. **Use Claude/Sonnet** for long conversations
2. **Upgrade to 8B model** (qwen3-8b-instruct) - may have better cache handling
3. **Upgrade GPU** to faster model (RTX 4090/H100)
4. **Implement context summarization** at application level

## Configuration Files

### Current Model Config (.env/models.json)
```json
"qwen2.5.1-coder-7b-instruct": {
  "type": "llama-cpp",
  "context": 131072,  // Model CLAIMS 131k
  "port": 8027,
  "maxTokens": 16384,
  "performance": {
    "flashAttention": true,
    "batch": 8192,
    "ubatch": 2048,
    "threads": 12,
    "cacheTypeK": "q4_0",
    "cacheTypeV": "q4_0"
  }
}
```

**Reality:** Model can only handle ~2k tokens effectively for generation.

### Recommended OpenClaw Config
```json
{
  "id": "lols-smart",
  "contextWindow": 3072,  // Safe limit
  "maxTokens": 1024
}
```

## Test Scripts

All tests available in `test/` directory:
- `test-2k-context.js` - Baseline (works perfectly)
- `test-10k-context.js` - Breaking point test
- `test-20k-context.js` - Confirms hang behavior
- `test-32k-context.js` - Upper limit test

Run any test:
```bash
node test/test-<size>-context.js
```

## Conclusion

The **qwen2.5.1-coder-7b-instruct** model on RTX 5060 Ti (16GB):
- ‚úÖ **Excellent for short conversations** (~2k tokens)
- ‚ùå **Cannot handle extended conversations** (>7k tokens)
- ‚ö†Ô∏è  **Requires strict context limits** in production

The advertised 131k context is **not usable** for interactive generation - only for prompt processing.
