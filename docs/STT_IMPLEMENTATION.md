# Speech-to-Text (STT) Implementation with whisper.cpp

> **Status**: ‚úÖ Implementation Complete | ‚è≥ Pending Testing & Deployment

This document describes the complete STT implementation for lols-router using whisper.cpp, with full GPU orchestration integration.

---

## üéØ Overview

**Added comprehensive STT support** with whisper.cpp integration, managing STT and LLM models through the same GPU mutex orchestrator. Only one model (LLM or STT) runs in VRAM at any given time.

### Key Features
- ‚úÖ **Whisper.cpp integration** - Native support for Whisper speech recognition models
- ‚úÖ **GPU orchestration** - Automatic model switching between LLM and STT
- ‚úÖ **OpenAI-compatible API** - `/v1/audio/transcriptions` endpoint
- ‚úÖ **Web UI support** - Audio file upload and transcription testing
- ‚úÖ **Automatic cleanup** - Temp audio files cleaned immediately after processing
- ‚úÖ **Multiple model sizes** - Small, Base, Medium models configured

---

## üìÅ Files Modified/Created

### New Files (3)
1. **`src/helpers/whisper.js`** - Whisper.cpp process management
2. **`src/endpoint/audio.js`** - Audio transcription API endpoint
3. **`docs/STT_IMPLEMENTATION.md`** - This documentation

### Modified Files (9)
1. **`models.json`** - Added whisper-small, whisper-base, whisper-medium
2. **`package.json`** - Added multer dependency
3. **`config.json`** - Added whisper binary and models paths
4. **`.env/config.json`** - Added whisper binary and models paths
5. **`src/helpers/orchestrator.js`** - Extended to handle whisper-cpp models
6. **`src/server.js`** - Imported and registered audio endpoint
7. **`src/webapp/index.html`** - Added audio upload UI
8. **`src/webapp/script.js`** - Added audio upload handling and transcription
9. **`src/webapp/style.css`** - Added audio upload styling

---

## üîß Implementation Details

### 1. Whisper.cpp Helper (`src/helpers/whisper.js`)

**Purpose**: Manage whisper-server processes similar to llama-server

**Key Functions**:
```javascript
startWhisper(cfg)        // Start whisper-server with model
stopWhisper(proc)        // Stop whisper process
waitReady(port)          // Wait for server to be ready
isWhisperOnPort(port)    // Check if whisper is running on port
```

**Configuration**:
- Binary path: `~/whisper.cpp/build/bin/whisper-server`
- Models dir: `~/whisper.cpp/models`
- Both configurable via `config.json`

**Process Management**:
- Spawns whisper-server with specified model file
- Supports language selection (auto-detect by default)
- Configurable thread count
- Health check endpoint: `/health`

---

### 2. GPU Orchestrator Updates (`src/helpers/orchestrator.js`)

**Changes**:
- Imported whisper helper functions
- Extended `ensureModel()` to handle `whisper-cpp` type
- Automatic model type detection and appropriate handler selection
- Unified GPU mutex lock for both LLM and STT models

**Model Switching Logic**:
```javascript
if (current && current.owned && current.proc) {
  // Stop LLM or STT based on current model type
  const stopType = current.type === "whisper-cpp" ? "whisper" : "llama";
  const stopFn = current.type === "whisper-cpp" ? stopWhisper : stopLlama;
  await withTimeout(stopFn(current.proc), 30000, `stop${stopType}`);
}
```

**Model Type Detection**:
```javascript
const modelType = model.type || "llama-cpp"; // Default to llama-cpp

if (modelType === "whisper-cpp") {
  // Start whisper process
} else {
  // Start llama process
}
```

**Benefits**:
- ‚úÖ Only one model in VRAM at a time
- ‚úÖ Automatic model switching (LLM ‚Üî STT)
- ‚úÖ Process adoption if already running
- ‚úÖ Ownership tracking (owned vs adopted)

---

### 3. Audio Transcription Endpoint (`src/endpoint/audio.js`)

**Endpoint**: `POST /v1/audio/transcriptions`

**OpenAI API Compatible**: Matches OpenAI Whisper API format

**Request Format**:
```bash
curl -X POST http://localhost:3000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "model=whisper-small" \
  -F "language=en" \
  -F "response_format=verbose_json"
```

**Parameters**:
- **file** (required): Audio file (mp3, wav, ogg, webm, m4a, flac)
- **model** (optional): Whisper model name (default: whisper-small)
- **language** (optional): Language code (default: auto)
- **prompt** (optional): Context prompt
- **temperature** (optional): Sampling temperature
- **response_format** (optional): `json` or `verbose_json`

**Response Format**:
```json
{
  "text": "Transcribed text here",
  "language": "en",
  "duration": 5.23,
  "segments": [...]
}
```

**Features**:
- ‚úÖ Multipart form data upload (multer)
- ‚úÖ File type validation (audio/* only)
- ‚úÖ File size limit (25 MB)
- ‚úÖ GPU orchestration (via `withGpu()`)
- ‚úÖ Automatic model loading
- ‚úÖ Temp file cleanup (immediate deletion after processing)
- ‚úÖ Error handling with OpenAI-compatible error format

**Temp File Handling**:
```javascript
try {
  tempFilePath = path.join(os.tmpdir(), `whisper-${rid}-${req.file.originalname}`);
  fs.writeFileSync(tempFilePath, req.file.buffer);
  // Process...
} finally {
  // Always cleanup, even on error
  if (tempFilePath && fs.existsSync(tempFilePath)) {
    fs.unlinkSync(tempFilePath);
  }
}
```

---

### 4. Web UI Updates

#### HTML (`src/webapp/index.html`)

Added audio upload section:
```html
<div class="form-group">
  <label for="audioInput">Upload Audio (for transcription):</label>
  <div class="audio-upload-container">
    <input type="file" id="audioInput" accept="audio/*" class="audio-input">
    <button id="clearAudioBtn" class="btn btn-secondary">Clear Audio</button>
    <button id="testTranscribeBtn" class="btn btn-primary">Transcribe</button>
  </div>
  <div id="audioPreview" class="audio-preview"></div>
</div>
```

#### JavaScript (`src/webapp/script.js`)

**State**:
```javascript
let uploadedAudio = null; // Stores File object
```

**Event Handlers**:
```javascript
handleAudioUpload(event)  // File selection and validation
clearAudio()              // Clear uploaded file
testTranscription()       // Test transcription with whisper
```

**Features**:
- File type validation (mp3, wav, ogg, webm, m4a, flac)
- File size validation (max 25 MB)
- Audio file preview (filename + size)
- Transcription button
- Result display in chat output
- Error handling and user feedback

#### CSS (`src/webapp/style.css`)

Added audio upload styling matching image upload:
```css
.audio-upload-container { /* flex layout */ }
.audio-input { /* dashed border, hover effects */ }
.audio-preview { /* preview box */ }
.audio-info { /* file info display */ }
.audio-size { /* file size styling */ }
```

---

### 5. Model Configuration (`models.json`)

Added three whisper models:

```json
{
  "whisper-small": {
    "type": "whisper-cpp",
    "repo": "ggerganov/whisper.cpp",
    "file": "ggml-small.bin",
    "port": 8030,
    "language": "auto"
  },
  "whisper-base": {
    "type": "whisper-cpp",
    "repo": "ggerganov/whisper.cpp",
    "file": "ggml-base.bin",
    "port": 8031,
    "language": "auto"
  },
  "whisper-medium": {
    "type": "whisper-cpp",
    "repo": "ggerganov/whisper.cpp",
    "file": "ggml-medium.bin",
    "port": 8032,
    "language": "auto"
  }
}
```

**Model Sizes**:
- **Base**: ~140 MB, fast, good for real-time
- **Small**: ~466 MB, balanced accuracy/speed
- **Medium**: ~1.5 GB, higher accuracy

**Port Allocation**:
- 8020-8024: LLM models
- 8030-8032: Whisper models

---

### 6. Configuration Files

#### `config.json` (root)
```json
{
  "whisper": {
    "bin": "~/whisper.cpp/build/bin/whisper-server",
    "models": "~/whisper.cpp/models"
  }
}
```

#### `.env/config.json` (remote)
Same structure as above.

**Path Resolution**:
- Tilde expansion supported (`~/`)
- Environment variables: `WHISPER_BIN`, `WHISPER_MODELS`
- Priority: config.json > env vars > defaults

---

## üîÑ GPU Orchestration Flow

### Scenario 1: LLM ‚Üí STT Switch

```
1. User has LLM model loaded (e.g., qwen2.5-7b-instruct on port 8020)
2. User uploads audio file and clicks "Transcribe"
3. Audio endpoint calls withGpu(async () => { ensureModel('whisper-small') })
4. Orchestrator acquires GPU lock
5. Orchestrator stops LLM model (if owned)
6. Orchestrator starts whisper-small on port 8030
7. Audio processed, transcription returned
8. GPU lock released
```

### Scenario 2: STT ‚Üí LLM Switch

```
1. User has Whisper model loaded (e.g., whisper-small on port 8030)
2. User sends chat message (e.g., to qwen2.5-7b-instruct)
3. Chat endpoint calls withGpu(async () => { ensureModel('qwen2.5-7b-instruct') })
4. Orchestrator acquires GPU lock
5. Orchestrator stops Whisper model (if owned)
6. Orchestrator starts LLM model on port 8020
7. Chat response generated
8. GPU lock released
```

### Scenario 3: Model Already Running

```
1. whisper-small already running on port 8030
2. Another transcription request comes in
3. Orchestrator checks: isWhisperOnPort(8030) ‚Üí true
4. Adopts existing process (owned: false)
5. No model switching needed
6. Request processed immediately
```

---

## üì¶ Dependencies

### New Dependency
- **multer** `^1.4.5-lts.1` - Multipart form data parsing for file uploads

**Note**: Multer 1.x has known vulnerabilities. Consider upgrading to 2.x in production.

---

## üß™ Testing Requirements

### Before Deployment

#### 1. Whisper.cpp Installation (Remote Server)
```bash
# Clone whisper.cpp
cd ~
git clone https://github.com/ggerganov/whisper.cpp
cd whisper.cpp

# Build with CUDA support
mkdir build
cd build
cmake .. -DGGML_CUDA=ON
cmake --build . --config Release

# Download models
cd ~/whisper.cpp
bash ./models/download-ggml-model.sh base
bash ./models/download-ggml-model.sh small
bash ./models/download-ggml-model.sh medium

# Verify installation
ls -lh ~/whisper.cpp/build/bin/whisper-server
ls -lh ~/whisper.cpp/models/ggml-*.bin
```

#### 2. GPU Configuration

**GPU support is enabled by default** for faster transcription. Configure in `config.json`:

```json
{
  "whisper": {
    "bin": "~/whisper.cpp/build/bin/whisper-server",
    "models": "~/whisper.cpp/models",
    "gpu": {
      "enabled": true,
      "device": 0
    }
  }
}
```

**Options**:
- `enabled` (boolean, default: `true`) - Enable/disable GPU acceleration
- `device` (number, default: `0`) - GPU device ID for multi-GPU systems

**Disable GPU** (CPU-only mode):
```json
{
  "whisper": {
    "gpu": {
      "enabled": false
    }
  }
}
```

**Verify GPU Usage**:
```bash
# Check if whisper is using GPU
nvidia-smi

# Should show whisper-server process using VRAM:
#   whisper-server    ~800-1000 MiB (small model)
```

**GPU Orchestration**:
- Only one model (LLM or STT) uses GPU at a time
- Orchestrator automatically swaps models based on request type
- Both models can coexist: one on GPU, others idle
- VRAM usage: ~800 MB (whisper-small), ~2-4 GB (LLM models)

#### 3. Local Testing
```bash
# Install dependencies
npm install

# Start server
npm start

# In browser: http://localhost:3000
# 1. Upload audio file
# 2. Click "Transcribe"
# 3. Verify transcription appears
# 4. Check logs for model switching
```

#### 4. Integration Tests

**Create**: `test/stt-test.js`

Test cases:
- ‚úÖ Whisper model loading
- ‚úÖ Audio file upload and transcription
- ‚úÖ LLM ‚Üí STT model switching
- ‚úÖ STT ‚Üí LLM model switching
- ‚úÖ Multiple transcriptions (model adoption)
- ‚úÖ Error handling (invalid file, missing model)
- ‚úÖ Temp file cleanup verification

#### 5. Remote Testing
```bash
# Deploy to remote
npm run deploy

# Run remote tests
npm run test:remote

# Manual testing via curl
curl -X POST http://192.168.0.21:3000/v1/audio/transcriptions \
  -F "file=@test.mp3" \
  -F "model=whisper-small"
```

---

## üîê Security & Privacy

### Audio File Handling
- ‚úÖ **Never persisted**: Audio files stored temporarily only during processing
- ‚úÖ **Immediate cleanup**: Deleted in `finally` block after transcription
- ‚úÖ **Memory efficient**: Multer stores in memory, written to temp only for whisper-server
- ‚úÖ **No logging**: Audio data never logged
- ‚úÖ **Size limits**: 25 MB max (OpenAI's limit)
- ‚úÖ **Type validation**: Only audio/* MIME types accepted

### Temp File Pattern
```
/tmp/whisper-<requestId>-<originalFilename>
```

**Lifespan**: <5 seconds typically
- Created: Just before whisper-server call
- Deleted: Immediately after transcription (finally block)
- Orphan cleanup: Handled by existing periodic cleanup (every 30 min)

---

## üìä VRAM Usage Estimates

### LLM Models
- **qwen2.5-1.5b-instruct**: ~2 GB
- **qwen2.5-7b-instruct**: ~6 GB
- **qwen2.5-14b-instruct**: ~10 GB
- **minicpm-v-2.6** (vision): ~8 GB

### Whisper Models
- **whisper-base**: ~500 MB
- **whisper-small**: ~1 GB
- **whisper-medium**: ~2-3 GB

### GPU Capacity (RTX 5060Ti 16GB)
- ‚úÖ **Any single LLM model**: Fits comfortably
- ‚úÖ **Any single Whisper model**: Fits easily
- ‚úÖ **Orchestrator ensures**: Only one model loaded at a time
- ‚úÖ **Model switching**: Frees VRAM before loading next

---

## üéì Usage Examples

### Web UI
```
1. Open http://localhost:3000
2. Scroll to "Upload Audio (for transcription)"
3. Click "Choose File" ‚Üí select audio file
4. Click "Transcribe"
5. Result appears in chat output
```

### cURL API
```bash
# Basic transcription
curl -X POST http://localhost:3000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "model=whisper-small"

# With language and verbose output
curl -X POST http://localhost:3000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "model=whisper-medium" \
  -F "language=en" \
  -F "response_format=verbose_json"

# With context prompt
curl -X POST http://localhost:3000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "model=whisper-small" \
  -F "prompt=This is a technical discussion about AI"
```

### Python Client
```python
import requests

url = "http://localhost:3000/v1/audio/transcriptions"

with open("audio.mp3", "rb") as f:
    files = {"file": f}
    data = {
        "model": "whisper-small",
        "language": "en",
        "response_format": "verbose_json"
    }
    response = requests.post(url, files=files, data=data)
    result = response.json()
    print(result["text"])
```

---

## üìù TODO: Testing & Deployment

### Immediate Next Steps
1. ‚è≥ **Install whisper.cpp on remote server**
   ```bash
   ssh ai@192.168.0.21
   cd ~
   git clone https://github.com/ggerganov/whisper.cpp
   cd whisper.cpp
   mkdir build && cd build
   cmake .. -DGGML_CUDA=ON
   cmake --build . --config Release
   cd ..
   bash ./models/download-ggml-model.sh small
   ```

2. ‚è≥ **Create integration tests**
   - File: `test/stt-integration-test.js`
   - Test whisper model loading
   - Test audio transcription
   - Test LLM ‚Üî STT model switching

3. ‚è≥ **Deploy and test remotely**
   ```bash
   npm run deploy
   npm run test:remote
   ```

4. ‚è≥ **Create test audio file**
   - Add `test/test-audio.mp3` for integration tests
   - 5-10 seconds, clear speech
   - English language for consistency

5. ‚è≥ **Update temp-cleanup patterns**
   - Add `whisper-*.mp3` to cleanup patterns
   - Ensure audio temp files are cleaned by periodic cleanup

6. ‚è≥ **Documentation updates**
   - Add STT to main README.md
   - Create WHISPER_SETUP.md
   - Update AGENTS.md with STT info
   - Add to docs/README.md index

### Future Enhancements
- [ ] Add whisper-large model support
- [ ] Streaming transcription (real-time)
- [ ] Speaker diarization
- [ ] Translation support (Whisper translate mode)
- [ ] Audio format conversion (ffmpeg integration)
- [ ] Web audio recording (record from mic in browser)
- [ ] Batch transcription endpoint

---

## üêõ Known Issues / Limitations

### Current Limitations
1. **No streaming**: Complete audio must be uploaded before processing
2. **Max file size**: 25 MB (OpenAI's limit)
3. **Multer 1.x**: Has known vulnerabilities (upgrade to 2.x recommended)
4. **No audio validation**: Only checks MIME type, not actual audio format
5. **Sequential processing**: One transcription at a time due to GPU lock

### Whisper.cpp Requirements
- **CUDA support**: Requires NVIDIA GPU with CUDA
- **Model files**: Must be downloaded manually
- **Binary path**: Must match config.json setting

---

## üìö Related Documentation

- [AGENTS.md](../AGENTS.md) - Main operational guide
- [IMAGE_HANDLING.md](IMAGE_HANDLING.md) - Similar file handling approach
- [VISION_SUMMARY.md](VISION_SUMMARY.md) - Vision model integration
- [test/README.md](../test/README.md) - Testing documentation

---

## ‚úÖ Implementation Checklist

### Completed
- [x] Create whisper.js helper
- [x] Update orchestrator for whisper-cpp
- [x] Create audio transcription endpoint
- [x] Add multer dependency
- [x] Update server.js routing
- [x] Add whisper config to config.json
- [x] Add whisper config to .env/config.json
- [x] Update models.json with whisper models
- [x] Add audio upload UI (HTML)
- [x] Add audio upload logic (JavaScript)
- [x] Add audio upload styling (CSS)
- [x] Create STT implementation documentation
- [x] Syntax validation of all new code

### Pending
- [ ] Install whisper.cpp on remote server
- [ ] Download whisper model files
- [ ] Create integration tests
- [ ] Test locally
- [ ] Deploy to remote
- [ ] Test remotely
- [ ] Update main documentation
- [ ] Create WHISPER_SETUP.md guide
- [ ] Add example audio file for tests
- [ ] Update AGENTS.md with STT info
- [ ] Update cleanup patterns for audio temp files

---

<div align="center">

**STT implementation complete! Ready for testing and deployment.** üéôÔ∏è‚ú®

[üè† Back to Docs](README.md) ‚Ä¢ [üìñ AGENTS.md](../AGENTS.md) ‚Ä¢ [üîß Setup Guide](../README.md)

</div>
