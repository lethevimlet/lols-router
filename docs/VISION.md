# Vision Model Support

> **Complete guide to multimodal image analysis with lols-router**

---

## üéØ Overview

**Status**: ‚úÖ Fully Implemented and Tested

lols-router supports vision models for multimodal AI interactions with image analysis capabilities. Vision requests are automatically detected and routed to specialized vision models, providing seamless image understanding alongside text-based interactions.

### Key Features

- ‚úÖ **Automatic vision detection** - Images in messages automatically trigger vision routing
- ‚úÖ **OpenAI-compatible API** - Standard multimodal message format
- ‚úÖ **Web UI support** - Drag-and-drop image upload with preview
- ‚úÖ **MiniCPM-V 2.6 integration** - Efficient vision model optimized for 16GB VRAM
- ‚úÖ **Seamless routing** - Works with `lols-smart` intelligent routing
- ‚úÖ **Comprehensive testing** - Integration tests included

---

## üöÄ Quick Start

### 1. Configuration

Vision model is already configured in `models.json`:

```json
{
  "lols-smart": {
    "vision": {
      "model": "minicpm-v-2.6",
      "systemPromptPath": "prompts/vision-expert.md"
    }
  },
  "models": {
    "minicpm-v-2.6": {
      "type": "llama-cpp",
      "repo": "openbmb/MiniCPM-V-2_6-gguf",
      "file": "ggml-model-Q4_K_M.gguf",
      "mmproj": "mmproj-model-f16.gguf",
      "context": 8192,
      "port": 8024,
      "supportsVision": true
    }
  }
}
```

### 2. Web UI Usage

1. Navigate to: **http://localhost:3000**
2. Click **"Upload Image"** and select an image
3. Enter prompt: "What do you see in this image?"
4. Select **"lols-smart"** (automatic routing) or **"minicpm-v-2.6"** (direct)
5. Click **"Send Test Request"**

### 3. API Usage

```bash
# Encode image to base64
IMAGE_BASE64=$(base64 -w 0 image.jpg)

curl http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"lols-smart\",
    \"messages\": [{
      \"role\": \"user\",
      \"content\": [
        {\"type\": \"text\", \"text\": \"What's in this image?\"},
        {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,$IMAGE_BASE64\"}}
      ]
    }],
    \"max_tokens\": 200
  }"
```

---

## üîß Technical Implementation

### Vision Model: MiniCPM-V 2.6

**Why MiniCPM-V 2.6?**
- Efficient for 16GB VRAM (Q4_K_M quantization ~4-5GB VRAM)
- Good balance of performance and resource usage
- Supports multimodal inputs (text + images)
- Open source with commercial-friendly license

**Model Specifications:**
- **Quantization**: Q4_K_M (balanced quality/performance)
- **VRAM Usage**: ~4-5GB
- **Context Window**: 8192 tokens
- **Port**: 8024
- **Format**: GGUF with mmproj (multimodal projector)
- **Download Size**: ~5.9GB (model + mmproj)

### Automatic Vision Detection

The router automatically detects images in message content:

```javascript
// From src/helpers/model-router.js
const hasImage = messages.some(msg => {
  if (Array.isArray(msg.content)) {
    return msg.content.some(part => 
      part.type === "image_url" || part.type === "image"
    );
  }
  return false;
});

if (hasImage) {
  log("Image content detected in request");
  return "vision";
}
```

### Multimodal Projector (mmproj)

Vision models require a multimodal projector to understand images:

```javascript
// From src/helpers/llama.js
if (cfg.mmproj) {
  args.push("--mmproj");
  args.push(`hf://${cfg.repo}/${cfg.mmproj}`);
  console.log("[llama] Starting vision model with mmproj:", cfg.mmproj);
}
```

- **Purpose**: Bridges vision and language representations
- **Download**: Automatic from HuggingFace on first use
- **Cache**: Stored in `~/.cache/llama.cpp/`
- **Size**: ~1.7GB (f16 precision)

---

## üé® Web Interface

### Image Upload Features

- **Drag-and-drop** or browse to upload
- **Image preview** with dimensions display
- **Format support**: PNG, JPEG, GIF, WebP, BMP
- **Base64 encoding** in browser (no server-side storage)
- **Clear button** to remove uploaded image

### Implementation Details

**HTML** (`src/webapp/index.html`):
```html
<div class="form-group">
  <label for="imageInput">Upload Image (optional):</label>
  <div class="image-upload-container">
    <input type="file" id="imageInput" accept="image/*">
    <button id="clearImageBtn">Clear Image</button>
  </div>
  <div id="imagePreview"></div>
</div>
```

**JavaScript** (`src/webapp/script.js`):
```javascript
// Image stored in browser memory only
let uploadedImage = null; // base64 data URL

// Multimodal message construction
if (uploadedImage) {
  messageContent = [
    { type: 'text', text: message },
    { type: 'image_url', image_url: { url: uploadedImage } }
  ];
}
```

**CSS** (`src/webapp/style.css`):
- Responsive design
- Visual feedback for upload state
- Preview with max dimensions
- Consistent styling with theme

---

## üì° API Reference

### Message Format

**OpenAI-compatible multimodal format:**

```json
{
  "model": "lols-smart",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Describe this image in detail"},
        {
          "type": "image_url",
          "image_url": {
            "url": "data:image/png;base64,iVBORw0KG..."
          }
        }
      ]
    }
  ],
  "max_tokens": 200,
  "temperature": 0.7,
  "stream": false
}
```

### Image Format Support

**Accepted formats:**
- PNG (`image/png`)
- JPEG (`image/jpeg`)
- GIF (`image/gif`)
- WebP (`image/webp`)
- BMP (`image/bmp`)

**Format requirements:**
- Base64-encoded data URL
- Pattern: `data:image/{type};base64,{encoded_data}`
- Maximum size: Determined by context window

### Response Format

Same as standard chat completions:

```json
{
  "id": "chatcmpl-xxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "minicpm-v-2.6",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "This image shows..."
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 50,
    "total_tokens": 200
  }
}
```

---

## üß™ Testing

### Integration Tests

**Test suite** (`test/integration-test.js`):
- ‚úÖ Vision model availability check
- ‚úÖ Vision category routing (automatic)
- ‚úÖ Vision model direct selection

**Run tests:**
```bash
# Full integration test suite (includes vision tests)
npm run test:integration

# Against remote server
node test/integration-test.js http://192.168.0.21:3000
```

### Test Image

Tests use a minimal 1x1 pixel test image:
```javascript
const TEST_IMAGE = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8DwHwAFBQIAX8jx0gAAAABJRU5ErkJggg==";
```

### Manual Testing

**Web UI:**
1. Start server: `npm start`
2. Open browser: http://localhost:3000
3. Upload test image
4. Send vision request
5. Verify response describes image

**API:**
```bash
# Test vision routing
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d @- << 'EOF'
{
  "model": "lols-smart",
  "messages": [{
    "role": "user",
    "content": [
      {"type": "text", "text": "What color is this?"},
      {"type": "image_url", "image_url": {"url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M/wHwAEBgIApD5fRAAAAABJRU5ErkJggg=="}}
    ]
  }]
}
EOF
```

---

## üìä Performance

### VRAM Usage

| Component | VRAM | Notes |
|-----------|------|-------|
| MiniCPM-V 2.6 (Q4_K_M) | ~4-5GB | Model + mmproj |
| Available for other models | 11-12GB | RTX 5060Ti 16GB total |

### Inference Speed

| Metric | Time | Notes |
|--------|------|-------|
| First request | 10-60s | Model download + loading |
| Subsequent requests | 5-30s | Model cached in VRAM |
| Model loading | 3-10s | VRAM allocation |
| Generation | 1-20s | Depends on image + prompt |

**Factors affecting speed:**
- Image resolution (higher = slower)
- Prompt complexity
- Context length
- Available VRAM
- GPU temperature/throttling

### Download Requirements

**First-time setup:**
- Model file: ~4.2GB (Q4_K_M GGUF)
- MMProj file: ~1.7GB (f16 projector)
- Total: ~5.9GB

Downloaded automatically by llama.cpp from HuggingFace on first use.

---

## üîç Category Detection

### Routing Priority

The router uses this priority for category detection:

1. **Vision** ‚Üê Images in message content **(HIGHEST)**
2. **Tools** ‚Üê Explicit tools/functions parameter
3. **Code** ‚Üê Programming tasks (routing model)
4. **Reason** ‚Üê Analysis tasks (routing model)
5. **Chat** ‚Üê Simple conversation (routing model)
6. **Default** ‚Üê Fallback **(LOWEST)**

### Vision Detection Logic

```javascript
// Automatic image detection (highest priority)
if (hasImageInMessages) {
  return { category: "vision", model: "minicpm-v-2.6" };
}

// Other categories determined by routing model
const category = await routingModel.classify(messages);
```

**Benefits:**
- No manual model selection needed
- Automatic fallback for text-only requests
- Consistent behavior across API and Web UI

---

## üìù System Prompts

### Vision Expert Prompt

**Location:** `prompts/vision-expert.md`

**Purpose:** Specialized instructions for image analysis

**Key guidelines:**
- Accurate and detailed image descriptions
- Object detection and localization
- Text recognition (OCR) when present
- Scene understanding and context
- Visual reasoning and analysis
- Safety and content moderation

**Example prompt structure:**
```markdown
You are a vision-language AI assistant specialized in analyzing images.

Your capabilities:
- Detailed image description
- Object detection and identification
- Text extraction (OCR)
- Scene understanding
- Visual reasoning

Guidelines:
- Be accurate and specific
- Describe what you actually see
- Avoid assumptions or speculation
- Note uncertainties explicitly
```

---

## üõ†Ô∏è Configuration

### Model Configuration

**File:** `models.json`

```json
{
  "lols-smart": {
    "vision": {
      "model": "minicpm-v-2.6",
      "systemPromptPath": "prompts/vision-expert.md"
    }
  },
  "models": {
    "minicpm-v-2.6": {
      "type": "llama-cpp",
      "repo": "openbmb/MiniCPM-V-2_6-gguf",
      "file": "ggml-model-Q4_K_M.gguf",
      "mmproj": "mmproj-model-f16.gguf",
      "context": 8192,
      "port": 8024,
      "systemPrompt": "You are a vision-language AI assistant...",
      "supportsVision": true
    }
  }
}
```

### Alternative Vision Models

If MiniCPM-V doesn't meet your needs, consider:

**LLaVA 1.5 (7B):**
```json
{
  "llava-1.5-7b": {
    "type": "llama-cpp",
    "repo": "mys/ggml_llava-v1.5-7b",
    "file": "ggml-model-q4_k.gguf",
    "mmproj": "mmproj-model-f16.gguf",
    "context": 4096,
    "port": 8025,
    "supportsVision": true
  }
}
```

**Qwen2-VL (7B):**
```json
{
  "qwen2-vl-7b": {
    "type": "llama-cpp",
    "repo": "Qwen/Qwen2-VL-7B-Instruct-GGUF",
    "file": "qwen2-vl-7b-instruct-q4_k_m.gguf",
    "mmproj": "mmproj-qwen2-vl-7b-instruct-f16.gguf",
    "context": 8192,
    "port": 8026,
    "supportsVision": true
  }
}
```

**Comparison:**

| Model | VRAM | Strengths | Weaknesses |
|-------|------|-----------|------------|
| MiniCPM-V 2.6 | ~5GB | Efficient, fast, good quality | Newer, less tested |
| LLaVA 1.5 | ~6GB | Well-tested, reliable | Slower, older architecture |
| Qwen2-VL | ~7GB | Best quality, multilingual | Higher VRAM, slower |

---

## üêõ Troubleshooting

### Vision Model Won't Load

**Symptom:** Model fails to start or times out

**Checks:**
```bash
# 1. Check VRAM availability
nvidia-smi

# 2. Verify llama.cpp supports multimodal
~/llama.cpp/build/bin/llama-server --help | grep -i mmproj

# 3. Check model files cached
ls -lh ~/.cache/llama.cpp/
```

**Solutions:**
- Free VRAM: Use "Kill Models" button or `/v1/cleanup` endpoint
- Rebuild llama.cpp with multimodal support
- Verify model files downloaded correctly
- Check port availability: `netstat -tlnp | grep 8024`

### Low Quality Responses

**Symptom:** Inaccurate or vague image descriptions

**Solutions:**
1. **Try higher quantization:**
   - Change `Q4_K_M` ‚Üí `Q5_K_M` or `Q6_K` in models.json
   - Higher quality but slower and more VRAM

2. **Increase context window:**
   - Change `"context": 8192` ‚Üí `"context": 16384`
   - More context for complex images

3. **Use higher resolution images:**
   - Resize images to higher resolution before upload
   - Balance with VRAM/speed constraints

4. **Adjust system prompt:**
   - Edit `prompts/vision-expert.md`
   - Add specific instructions for your use case

### Slow Inference

**Symptom:** Vision requests take 30+ seconds

**Expected behavior:**
- Vision models are inherently slower than text-only
- First request downloads model (~6GB)
- Subsequent requests use cached model

**Optimizations:**
```json
{
  "minicpm-v-2.6": {
    "context": 4096,  // Reduce from 8192
    "file": "ggml-model-Q4_K_S.gguf"  // Faster quantization
  }
}
```

**Other solutions:**
- Reduce image resolution before upload
- Use smaller vision model (LLaVA 1.5 base instead of 7B)
- Ensure GPU not being used by other processes
- Check GPU temperature (thermal throttling)

### Images Not Detected

**Symptom:** Image requests routed to text model

**Debug:**
```bash
# Check server logs
tail -f server.log | grep -i "vision\|image"

# Test image detection endpoint
curl http://localhost:3000/v1/test/model \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{
      "role": "user",
      "content": [
        {"type": "text", "text": "test"},
        {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
      ]
    }]
  }'
```

**Solutions:**
- Verify message format matches OpenAI spec
- Check `type: "image_url"` is present
- Ensure base64 data URL format: `data:image/{type};base64,{data}`
- Review router logs for detection logic

### VRAM Out of Memory

**Symptom:** Model fails to load with CUDA OOM error

**Solutions:**
1. **Free existing models:**
   ```bash
   curl -X POST http://localhost:3000/v1/cleanup
   ```

2. **Use smaller quantization:**
   - Q4_K_M ‚Üí Q3_K_M (lower quality, less VRAM)

3. **Reduce context window:**
   - 8192 ‚Üí 4096 or 2048

4. **Use smaller vision model:**
   - MiniCPM-V 2.6 ‚Üí LLaVA 1.5 base

5. **GPU orchestration working?**
   - Check logs show "stopping LLM before loading vision"
   - Verify only one model in VRAM at a time

---

## üìö Files Modified

### Configuration
- ‚úÖ `models.json` - Vision category and model
- ‚úÖ `.env/models.json` - Remote environment config

### System Prompts
- ‚úÖ `prompts/vision-expert.md` - Vision-specific instructions
- ‚úÖ `.env/prompts/vision-expert.md` - Remote copy

### Backend
- ‚úÖ `src/helpers/model-router.js` - Image detection and routing
- ‚úÖ `src/helpers/llama.js` - mmproj parameter support

### Frontend
- ‚úÖ `src/webapp/index.html` - Image upload UI
- ‚úÖ `src/webapp/script.js` - Image handling and encoding
- ‚úÖ `src/webapp/style.css` - Image upload styling

### Tests
- ‚úÖ `test/integration-test.js` - 3 vision tests added
- ‚úÖ `test/vision-test.js` - Vision detection logic tests

### Documentation
- ‚úÖ `docs/VISION.md` - This comprehensive guide
- ‚úÖ `docs/IMAGE_HANDLING.md` - Privacy and storage details

---

## üîê Privacy & Security

### Image Handling

See [IMAGE_HANDLING.md](IMAGE_HANDLING.md) for complete details.

**Key points:**
- ‚úÖ **Never persisted to disk** - Processed in memory only
- ‚úÖ **Immediate cleanup** - Discarded after response
- ‚úÖ **No logging** - Image data never logged
- ‚úÖ **GDPR compliant** - No data retention
- ‚úÖ **Client-side encoding** - Base64 conversion in browser

**Flow:**
```
Browser ‚Üí Base64 encode ‚Üí JSON request ‚Üí Server memory ‚Üí
llama-server VRAM ‚Üí Response ‚Üí Garbage collected
```

### Best Practices

1. **Don't log image data:**
   - Logs should contain metadata only
   - Never log base64 strings

2. **Client-side validation:**
   - Check file types before upload
   - Limit file sizes (e.g., 10MB max)

3. **Content moderation:**
   - Vision prompt includes safety guidelines
   - Consider adding content filtering

4. **User privacy:**
   - Inform users images aren't stored
   - No image analytics or tracking

---

## üìà Next Steps

### Recommended Testing Workflow

1. ‚úÖ **Local testing** - Verify vision works on your machine
2. üß™ **Integration tests** - Run full test suite
3. üì° **Remote deployment** - Deploy to server with GPU
4. üåê **Web UI testing** - Test with real images
5. üìä **Performance monitoring** - Watch GPU usage and speed

### Optional Enhancements

**Model improvements:**
- Try alternative vision models (LLaVA, Qwen2-VL)
- Experiment with quantization levels
- Fine-tune system prompts for your use case

**Feature additions:**
- Multiple images per message
- Image preprocessing (resize, crop, rotate)
- Streaming support for vision responses
- Image format validation
- Image caching for repeat requests

**UI enhancements:**
- Image editor integration
- Batch image upload
- Image history/gallery
- OCR-specific mode

---

## üéâ Summary

**Vision model support is fully implemented and production-ready!**

### What Works

- ‚úÖ Automatic vision detection from message content
- ‚úÖ OpenAI-compatible multimodal API format
- ‚úÖ Web UI with drag-and-drop image upload
- ‚úÖ MiniCPM-V 2.6 optimized for 16GB VRAM
- ‚úÖ Seamless integration with lols-smart routing
- ‚úÖ Comprehensive testing (integration + unit tests)
- ‚úÖ Privacy-focused (no image persistence)

### Performance Profile

- **VRAM**: ~5GB for vision model
- **Speed**: 5-30s per request (after initial download)
- **Quality**: Good balance with Q4_K_M quantization
- **Context**: 8192 tokens (configurable)

### Getting Started

1. Start server: `npm start`
2. Open UI: http://localhost:3000
3. Upload image and send request
4. Vision model loads automatically
5. Response describes your image

**Ready to use!** üöÄ

---

## üìñ Related Documentation

- [IMAGE_HANDLING.md](IMAGE_HANDLING.md) - Privacy and storage details
- [REMOTE_TESTING.md](REMOTE_TESTING.md) - Deployment guide
- [STT_IMPLEMENTATION.md](STT_IMPLEMENTATION.md) - Speech-to-text features
- [Main README](../README.md) - Complete project overview
- [AGENTS.md](../AGENTS.md) - Development guide

---

<div align="center">

**Vision support complete!** üëÅÔ∏è‚ú®

[üè† Back to Docs](README.md) ‚Ä¢ [üì° API Reference](API_REFERENCE.md) ‚Ä¢ [üí¨ Examples](EXAMPLES.md)

</div>
